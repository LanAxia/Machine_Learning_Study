{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最基础的全连接网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNetwork(object):\n",
    "    def __init__(self, shape, **paras):\n",
    "        self.shape = tuple(shape) # 全连接网络的机构\n",
    "        self.deep = len(shape) # 网络的层数\n",
    "        self.layer_id = tuple(range(self.deep))\n",
    "\n",
    "        self.weights = {i:np.matrix(np.random.randn(self.shape[i], self.shape[i - 1])) for i in self.layer_id[1:]} # 随机初始化weights\n",
    "        self.bias = {i:np.matrix(np.random.randn(self.shape[i], 1)) for i in self.layer_id[1:]}\n",
    "        self.d_z = {i:None for i in self.layer_id[1:]}\n",
    "        self.a = {i:None for i in self.layer_id}\n",
    "        self.z = {i:None for i in self.layer_id[1:]}\n",
    "        self.y_pred = None\n",
    "    \n",
    "    def forward_propagation(self, train_data_mat):\n",
    "        self.a[0] = train_data_mat\n",
    "        for i in self.layer_id[1:]:\n",
    "            self.z[i] = self.weights[i] * self.a[i - 1] + self.bias[i] # z=wx + b # broadcast\n",
    "            self.a[i] = self.activate(self.z[i])\n",
    "        \n",
    "        y_pred_mat = np.matrix(self.a[self.layer_id[-1]])\n",
    "        self.y_pred = y_pred_mat\n",
    "        return y_pred_mat\n",
    "\n",
    "    def back_propagation(self, train_label_mat, y_pred):\n",
    "        d_l = self.cal_loss_derivation(y_pred, train_label_mat) # 计算loss\n",
    "        self.d_z[self.layer_id[-1]] = d_l\n",
    "        for i in sorted(self.layer_id[1: -1], reverse=True):\n",
    "            d_a_i = self.weights[i + 1].T * self.d_z[i + 1] # wixwi+1 * wi+1xb\n",
    "            d_z_i = np.multiply(self.activate_derivation(self.z[i]), d_a_i)\n",
    "            self.d_z[i] = d_z_i\n",
    "    \n",
    "    def update_paras(self, alpha=2):\n",
    "        for i in self.layer_id[1:]:\n",
    "            w_gradient = self.d_z[i] * self.a[i - 1].T\n",
    "            self.weights[i] = self.weights[i] - alpha * w_gradient / self.d_z[i].shape[1]\n",
    "\n",
    "            b_gradient = self.d_z[i]\n",
    "            self.bias[i] = self.bias[i] - alpha * np.average(b_gradient, axis=1)\n",
    "    \n",
    "    def fit(self, train_data, train_label, max_round=100, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            data_index = list(range(len(train_data)))\n",
    "            for round in range(max_round):\n",
    "                np.random.shuffle(data_index)\n",
    "                for i in np.data_index:\n",
    "                    alpha=0.2\n",
    "                    train_data_mat = np.matrix(train_data[i]).T # 转置为n*b\n",
    "                    train_label_mat = np.matrix(train_label[i]).T # 转置为y*b\n",
    "                    y_pred = self.forward_propagation(train_data_mat)\n",
    "                    self.back_propagation(train_label_mat, y_pred)\n",
    "                    self.update_paras(alpha)\n",
    "        else: # 小批量（批量）更新\n",
    "            data_index = list(range(len(train_data)))\n",
    "            for round in range(max_round):\n",
    "                np.random.shuffle(data_index)\n",
    "                for i in range(math.floor(len(train_data) / batch_size) + 1):\n",
    "                    batch_index = data_index[batch_size * i : batch_size * (i + 1)]\n",
    "                    if len(batch_index) > 0:\n",
    "                        alpha = 0.01\n",
    "                        batch_data_mat = np.matrix(train_data[batch_index].T)\n",
    "                        batch_label_mat = np.matrix(train_label[batch_index].T)\n",
    "\n",
    "                        y_pred = self.forward_propagation(batch_data_mat)\n",
    "                        self.back_propagation(batch_label_mat, y_pred)\n",
    "                        self.update_paras(alpha)\n",
    "\n",
    "    def predict(self, test_data_vec):\n",
    "        # 将向量转换为矩阵便于计算\n",
    "        test_data_mat = np.matrix(test_data_vec.reshape((self.shape[0], -1)))\n",
    "\n",
    "        a = test_data_mat\n",
    "        for i in self.layer_id[1:]:\n",
    "            z = self.weights[i] * a + self.bias[i] # z=wx + b\n",
    "            a = self.activate(z)\n",
    "        \n",
    "        y_pred_mat = np.matrix(a)\n",
    "        return y_pred_mat\n",
    "    \n",
    "    def activate(self, z):\n",
    "        a = self.sigmoid(z)\n",
    "        return a\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(mat):\n",
    "        x = np.array(mat).ravel()                                # 铺平\n",
    "        y = []\n",
    "        for i in range(len(x)):\n",
    "            if  x[i] >= 0:\n",
    "                y.append(1 / (1 + np.exp(-x[i])))\n",
    "            else:\n",
    "                y.append(np.exp(x[i]) / (1 + np.exp(x[i])))      # 当某一个元素小于0时，用另一个公式计算，解决上溢问题\n",
    "        return np.matrix(np.array(y).reshape(mat.shape))\n",
    "    \n",
    "    def activate_derivation(self, z):\n",
    "        da_dz = self.sigmoid_derivation(z)\n",
    "        return da_dz\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivation(z):\n",
    "        sigmoid_d = np.multiply(FullyConnectedNetwork.sigmoid(z), 1 - FullyConnectedNetwork.sigmoid(z))\n",
    "        return sigmoid_d\n",
    "    \n",
    "    def cal_loss_derivation(self, y_pred, y_real):\n",
    "        d_l = y_pred - y_real\n",
    "        return d_l=\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具有完善功能的全连接网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    def __init__(self, shape, activation, loss_function='binary_cross_entrophy', **paras):\n",
    "        self.shape = tuple(shape) # 全连接网络的机构\n",
    "        self.deep = len(shape) # 网络的层数\n",
    "        self.layer_id = tuple(range(self.deep))\n",
    "        self.activation = {id : activation[id - 1] for id in self.layer_id[1:]}  # 从第二层开始有激活函数\n",
    "        self.loss_function = loss_function\n",
    "        if self.loss_function == 'softmax_ce':\n",
    "            self.activation[self.layer_id[-1]] = 'softmax'\n",
    "\n",
    "        self.weights = {i:np.matrix(np.random.randn(self.shape[i], self.shape[i - 1])) \\\n",
    "            for i in self.layer_id[1:]} # 随机初始化weights\n",
    "        self.bias = {i:np.matrix(np.random.randn(self.shape[i], 1)) for i in self.layer_id[1:]}\n",
    "        self.d_z = {i:None for i in self.layer_id[1:]}\n",
    "        self.a = {i:None for i in self.layer_id}\n",
    "        self.z = {i:None for i in self.layer_id[1:]}\n",
    "        self.y_pred = None\n",
    "    \n",
    "    # 前向传播\n",
    "    def forward_propagation(self, train_data_mat):\n",
    "        self.a[0] = train_data_mat  # n*bs\n",
    "        for i in self.layer_id[1:]:\n",
    "            self.z[i] = self.weights[i] * self.a[i - 1] + self.bias[i]  # z=wx + b # broadcast # wi*bs\n",
    "            self.a[i] = self.activate(self.z[i], i)  # wi*bs\n",
    "        \n",
    "        y_pred_mat = np.matrix(self.a[self.layer_id[-1]])\n",
    "        self.y_pred = y_pred_mat\n",
    "        return y_pred_mat\n",
    "\n",
    "    # 反向传播\n",
    "    def back_propagation(self, train_label_mat, y_pred):\n",
    "        d_l_a = self.cal_loss_derivation(y_pred, train_label_mat) # 计算loss\n",
    "        d_l_z = np.multiply(self.activate_derivation(self.z[self.layer_id[-1]], \\\n",
    "            self.layer_id[-1]), d_l_a)\n",
    "        self.d_z[self.layer_id[-1]] = d_l_z  # 获取输出层的dz\n",
    "        for i in sorted(self.layer_id[1: -1], reverse=True):\n",
    "            d_a_i = self.weights[i + 1].T * self.d_z[i + 1] # wixwi+1 * wi+1xb\n",
    "            d_z_i = np.multiply(self.activate_derivation(self.z[i], i), d_a_i)\n",
    "            self.d_z[i] = d_z_i\n",
    "    \n",
    "    # 更新权重\n",
    "    def update_paras(self, alpha=2):\n",
    "        for i in self.layer_id[1:]:\n",
    "            w_gradient = self.d_z[i] * self.a[i - 1].T\n",
    "            self.weights[i] = self.weights[i] - alpha * w_gradient\n",
    "\n",
    "            b_gradient = self.d_z[i]\n",
    "            self.bias[i] = self.bias[i] - alpha * np.sum(b_gradient, axis=1)\n",
    "    \n",
    "    def fit(self, train_data, train_label, epochs=100, batch_size=1, \\\n",
    "        learning_rate=0.01, decay=1e-6):  # s*n\n",
    "        data_index = list(range(train_data.shape[0]))\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(data_index)\n",
    "            for i in range(math.floor(train_data.shape[0] / batch_size) + 1):\n",
    "                batch_index = data_index[batch_size * i : batch_size * (i + 1)]  # bs*n\n",
    "                if len(batch_index) > 0:\n",
    "                    alpha = max((0, learning_rate - epoch * decay))\n",
    "                    batch_data_mat = np.matrix(train_data[batch_index].T)  # n*bs\n",
    "                    batch_label_mat = np.matrix(train_label[batch_index].T)  # y*bs\n",
    "\n",
    "                    y_pred = self.forward_propagation(batch_data_mat)\n",
    "                    self.back_propagation(batch_label_mat, y_pred)\n",
    "                    self.update_paras(alpha)\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        # 将向量转换为矩阵便于计算\n",
    "        a = np.matrix(test_data.T)\n",
    "        for i in self.layer_id[1:]:\n",
    "            z = self.weights[i] * a + self.bias[i] # z=wx + b\n",
    "            a = self.activate(z, i)\n",
    "        return a\n",
    "    \n",
    "    def activate(self, z, layer_id):\n",
    "        activation_method = self.activation[layer_id]\n",
    "        if activation_method == 'sigmoid':\n",
    "            a = self.sigmoid(z)\n",
    "        elif activation_method == 'tanh':\n",
    "            a = self.tanh(z)\n",
    "        elif activation_method == 'relu':\n",
    "            a = self.relu(z)\n",
    "        elif activation_method == 'softmax':\n",
    "            a = self.softmax(z)\n",
    "        return a\n",
    "    \n",
    "    def activate_derivation(self, z, layer_id):\n",
    "        activation_method = self.activation[layer_id]\n",
    "        if activation_method == 'sigmoid':\n",
    "            da_dz = self.sigmoid_derivation(z)\n",
    "        elif activation_method == 'tanh':\n",
    "            da_dz = self.tanh_derivation(z)\n",
    "        elif activation_method == 'relu':\n",
    "            da_dz = self.relu_derivation(z)\n",
    "        elif activation_method == 'softmax':\n",
    "            da_dz = 1  # 在损失函数中直接计算好da_dz\n",
    "        else:\n",
    "            return None\n",
    "        return da_dz\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(mat):\n",
    "        mat = mat.copy()\n",
    "        x = np.array(mat).ravel()                                # 铺平\n",
    "        y = []\n",
    "        for i in range(len(x)):\n",
    "            if  x[i] >= 0:\n",
    "                y.append(1 / (1 + np.exp(-x[i])))\n",
    "            else:\n",
    "                y.append(np.exp(x[i]) / (1 + np.exp(x[i])))      # 当某一个元素小于0时，用另一个公式计算，解决上溢问题\n",
    "        return np.matrix(np.array(y).reshape(mat.shape))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivation(z):\n",
    "        z = z.copy()\n",
    "        sigmoid_d = np.multiply(DNN.sigmoid(z), 1 - DNN.sigmoid(z))\n",
    "        return sigmoid_d\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(mat):\n",
    "        mat = mat.copy()\n",
    "        result = (np.exp(mat) - np.exp(- mat)) / (np.exp(mat) + np.exp(- mat))\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh_derivation(z):\n",
    "        z = z.copy()\n",
    "        result = 1 - np.power(DNN.tanh(z), 2)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(mat):\n",
    "        mat = mat.copy()\n",
    "        mat[mat < 0] = 0\n",
    "        return mat\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivation(z):\n",
    "        z = z.copy()\n",
    "        z[z < 0] = 0\n",
    "        z[z >= 0] = 1\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(mat):\n",
    "        mat = mat.copy()\n",
    "        mat = np.exp(mat)\n",
    "        mat_sum = np.sum(mat, axis=0)\n",
    "        result = np.divide(mat + 1e-7, mat_sum + 1e-7)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_derivation(z):\n",
    "        pass\n",
    "    \n",
    "    def cal_loss_derivation(self, y_pred, y_real):\n",
    "        if self.loss_function == 'binary_cross_entrophy':\n",
    "            d_l = self.binary_cross_entrophy_derivation(y_pred, y_real)\n",
    "        elif self.loss_function == 'cross_entrophy':\n",
    "            d_l = self.cross_entrophy_derivation(y_pred, y_real)\n",
    "        elif self.loss_function == 'softmax_ce':\n",
    "            d_l = self.softmax_ce_derivation(y_pred, y_real)\n",
    "        else:\n",
    "            return None\n",
    "        return d_l\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_cross_entrophy_derivation(y_pred, y_real):\n",
    "        result = (- np.divide(y_real + 1e-7, y_pred + 1e-7)) + \\\n",
    "            (np.divide(1 - y_real - 1e-7, 1 - y_pred - 1e-7))\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entrophy_derivation(y_pred, y_real):\n",
    "        result = - np.divide(y_real + 1e-7, y_pred + 1e-7)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_ce_derivation(y_pred, y_real):\n",
    "        result = y_pred - y_real\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_accuracy(y_pred, y_real):\n",
    "    right_num = 0\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        i_pred = np.argmax(y_pred.T[i])\n",
    "        i_real = np.argmax(y_real.T[i])\n",
    "        if i_pred == i_real:\n",
    "            right_num += 1\n",
    "    return right_num / y_pred.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有手写数字训练集：1934组\n",
      "\n",
      "共有手写数字测试集：946组\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "导入数据集\n",
    "'''\n",
    "filename_ptn = re.compile('(\\d)_\\d+.txt')\n",
    "train_path = './Data/trainingDigits'\n",
    "test_path = './Data/testDigits'\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for filename in os.listdir(train_path):\n",
    "    file = filename_ptn.search(filename)\n",
    "    if file:\n",
    "        file_path = os.path.join(train_path, filename)\n",
    "        with open(file_path, 'r+') as f:\n",
    "            x_train.append(f.read())\n",
    "        y_train.append(file.group(1))\n",
    "\n",
    "for filename in os.listdir(test_path):\n",
    "    file = filename_ptn.search(filename)\n",
    "    if file:\n",
    "        file_path = os.path.join(test_path, filename)\n",
    "        with open(file_path, 'r+') as f:\n",
    "            x_test.append(f.read())\n",
    "        y_test.append(file.group(1))\n",
    "\n",
    "\n",
    "print('共有手写数字训练集：{}组\\n'.format(len(x_train)))\n",
    "print('共有手写数字测试集：{}组'.format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集特征的维度：(1934, 1024)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "基于全连接网络对手写数字数据进行适当的处理\n",
    "'''\n",
    "def proceed_ann_x(x_lst):\n",
    "    x_lst = x_lst.copy()\n",
    "    for i, x in enumerate(x_lst):\n",
    "        digit = ''.join(x.split('\\n'))\n",
    "        x_lst[i] = np.array([int(px) for px in digit]).reshape((1, -1))\n",
    "    x_array = np.concatenate(x_lst)\n",
    "    return x_array\n",
    "\n",
    "x_train_ann = proceed_ann_x(x_train)\n",
    "x_test_ann = proceed_ann_x(x_test)\n",
    "\n",
    "print('训练集特征的维度：{}'.format(x_train_ann.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集标签集的维度：(1934, 10)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "对labels进行处理\n",
    "'''\n",
    "def proceed_y(y_lst):\n",
    "    for i, y in enumerate(y_lst):\n",
    "        label = np.zeros((1, 10))\n",
    "        label[0, int(y)] = 1\n",
    "        y_lst[i] = label\n",
    "    y_array = np.concatenate(y_lst)\n",
    "    return y_array\n",
    "\n",
    "y_train = proceed_y(y_train)\n",
    "y_test = proceed_y(y_test)\n",
    "\n",
    "print('训练集标签集的维度：{}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_accuracy(y_pred, y_real):\n",
    "    right_num = 0\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        i_pred = np.argmax(y_pred.T[i])\n",
    "        i_real = np.argmax(y_real.T[i])\n",
    "        if i_pred == i_real:\n",
    "            right_num += 1\n",
    "    return right_num / y_pred.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "K-Fold交叉验证\n",
    "'''\n",
    "def k_fold(data, labels, k, random_shuffle=True):\n",
    "    data, labels = data.copy(), labels.copy()\n",
    "    data_index = list(range(data.shape[0]))\n",
    "\n",
    "    if random_shuffle == True:\n",
    "        np.random.shuffle(data_index)\n",
    "    \n",
    "    for i in range(k):\n",
    "        fold_size = int(np.floor(data.shape[0] / k) + 1)\n",
    "        test_index = data_index[fold_size * i:fold_size * (i + 1)]\n",
    "        train_index = data_index[:fold_size * i] + data_index[fold_size * (i + 1):]\n",
    "        yield data[train_index], labels[train_index], data[test_index], labels[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经过100epochs的迭代：\n",
      "该神经网络在训练集上的准确率为：99.90%\n",
      "----------------------------------------\n",
      "经过100epochs的迭代：\n",
      "该神经网络在测试集上的准确率为：90.70%\n"
     ]
    }
   ],
   "source": [
    "model = DNN((1024, 128, 32, 10), ('sigmoid', 'sigmoid', 'softmax'), loss_function='softmax_ce')\n",
    "model.fit(x_train_ann, y_train, epochs=50, learning_rate=0.01, decay=1e-6, batch_size=50)\n",
    "\n",
    "y_pred = model.predict(x_train_ann)\n",
    "cat_acc = cat_accuracy(y_pred, y_train.T)\n",
    "print('经过100epochs的迭代：')\n",
    "print('该神经网络在训练集上的准确率为：{:.02f}%'.format(cat_acc * 100))\n",
    "\n",
    "print('----------------------------------------')\n",
    "y_pred = model.predict(x_test_ann)\n",
    "cat_acc = cat_accuracy(y_pred, y_test.T)\n",
    "print('经过100epochs的迭代：')\n",
    "print('该神经网络在测试集上的准确率为：{:.02f}%'.format(cat_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经过K Fold交叉验证：\n",
      "K: 10\n",
      "准确率：90.18%\n"
     ]
    }
   ],
   "source": [
    "fold_num = 10\n",
    "accuracy = 0\n",
    "for x_t, y_t, x_v, y_v in k_fold(x_train_ann, y_train, fold_num):\n",
    "    model = DNN((1024, 128, 32, 10), ('sigmoid', 'sigmoid', 'softmax'), loss_function='softmax_ce')\n",
    "    x_t, y_t, x_v, y_v = x_t, y_t, x_v, y_v\n",
    "    model.fit(x_t, y_t, epochs=50, learning_rate=0.01, decay=1e-6, batch_size=50)\n",
    "    pred_result = model.predict(x_v)\n",
    "    accuracy += cat_accuracy(pred_result, np.matrix(y_v).T)\n",
    "\n",
    "print('经过K Fold交叉验证：')\n",
    "print('K: {}'.format(fold_num))\n",
    "print('准确率：{:.02f}%'.format(accuracy / fold_num * 100))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b0090f9c919a3d6ed76b4d901c1278536bd598516fd1e0ec891bddcf94e37ee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
